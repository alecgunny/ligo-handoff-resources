{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports, function defs, some light dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure\n",
    "from hermes.aeriel.client import InferenceClient\n",
    "from hermes.aeriel.serve import serve\n",
    "from hermes import quiver as qv\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import aframe.architectures\n",
    "import export.main\n",
    "from export.snapshotter import BackgroundSnapshotter, BatchWhitener\n",
    "\n",
    "# in case you need to make any changes to the code\n",
    "# and want them reflected without having to restart\n",
    "# the notebook\n",
    "export = reload(export.main)\n",
    "architectures = reload(aframe.architectures)\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "DATA_DIR = Path.home() / \"bbhnet\" / \"data\" / \"test\" / \"background\"\n",
    "IFOS = [\"H1\", \"L1\"]\n",
    "SAMPLE_RATE = 2048\n",
    "PSD_LENGTH = 64\n",
    "KERNEL_LENGTH = 1.5\n",
    "FDURATION = 1\n",
    "HIGHPASS = 32\n",
    "\n",
    "NUM_IFOS = len(IFOS)\n",
    "FFTLENGTH = KERNEL_LENGTH + FDURATION\n",
    "KERNEL_SIZE = int(KERNEL_LENGTH * SAMPLE_RATE)\n",
    "SNAPSHOT_SIZE = int(FFTLENGTH * SAMPLE_RATE)\n",
    "\n",
    "fnames = list(DATA_DIR.iterdir())\n",
    "data = []\n",
    "with h5py.File(fnames[0], \"r\") as f:\n",
    "    for ifo in IFOS:\n",
    "        data.append(f[ifo][:])\n",
    "data = np.stack(data)\n",
    "length = data.shape[-1] / SAMPLE_RATE\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def do_local_inference(nn, batch_size, inference_sampling_rate):\n",
    "    snapshotter = BackgroundSnapshotter(\n",
    "        PSD_LENGTH,\n",
    "        KERNEL_LENGTH,\n",
    "        FDURATION,\n",
    "        SAMPLE_RATE,\n",
    "        inference_sampling_rate,\n",
    "    ).to(\"cuda\")\n",
    "    whitener = BatchWhitener(\n",
    "        KERNEL_LENGTH,\n",
    "        SAMPLE_RATE,\n",
    "        inference_sampling_rate,\n",
    "        batch_size,\n",
    "        FDURATION,\n",
    "        FFTLENGTH,\n",
    "        HIGHPASS\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    state = torch.zeros((1, NUM_IFOS, snapshotter.state_size)).to(\"cuda\")\n",
    "    integration_size = int(KERNEL_LENGTH * inference_sampling_rate)\n",
    "    step_size = int(batch_size * SAMPLE_RATE / inference_sampling_rate)\n",
    "\n",
    "    num_steps = (data.shape[-1] - SNAPSHOT_SIZE) // step_size + 1\n",
    "    start_time = time.time()\n",
    "    latencies = np.zeros(num_steps)\n",
    "    for i in trange(num_steps):\n",
    "        tick = time.time()\n",
    "        update = data[:, i * step_size: (i + 1) * step_size]\n",
    "        update = torch.Tensor(update).to(\"cuda\").view(1, NUM_IFOS, -1)\n",
    "\n",
    "        x, state = snapshotter(update, state)\n",
    "        x = whitener(x)\n",
    "        y = nn(x).cpu().numpy()[0, 0]\n",
    "\n",
    "        tock = time.time()\n",
    "        latencies[i] = tock - tick\n",
    "\n",
    "    end_time = time.time()\n",
    "    throughput = length / (end_time - start_time)\n",
    "    return throughput, latencies\n",
    "\n",
    "\n",
    "def configure_deployment(\n",
    "    batch_size: int,\n",
    "    inference_sampling_rate: float,\n",
    "    instances: int\n",
    "):\n",
    "    export.main(\n",
    "        lambda num_ifos: architectures.ResNet(num_ifos, layers=[3, 4, 4, 3], norm_groups=16),\n",
    "        repository_directory=Path(\"model_repo\"),\n",
    "        logdir=Path(\".\"),\n",
    "        num_ifos=NUM_IFOS,\n",
    "        kernel_length=KERNEL_LENGTH,\n",
    "        inference_sampling_rate=inference_sampling_rate,\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        batch_size=batch_size,\n",
    "        fduration=FDURATION,\n",
    "        psd_length=PSD_LENGTH,\n",
    "        fftlength=KERNEL_LENGTH + FDURATION,\n",
    "        highpass=32,\n",
    "        streams_per_gpu=1,\n",
    "        aframe_instances=instances,\n",
    "        platform=qv.Platform.TENSORRT,\n",
    "        clean=True\n",
    "    )\n",
    "\n",
    "\n",
    "def do_inference(batch_size, inference_sampling_rate):\n",
    "    step_size = batch_size * int(SAMPLE_RATE / inference_sampling_rate)\n",
    "    num_steps = (data.shape[-1] - SNAPSHOT_SIZE) // step_size + 1\n",
    "\n",
    "    num_steps = min(num_steps, 10000)\n",
    "    total_time = num_steps * step_size / SAMPLE_RATE\n",
    "\n",
    "    ctx = serve(\"model_repo\", image=\"hermes/tritonserver:22.12\", gpus=[0], wait=True)\n",
    "    pbar = tqdm(total=total_time)\n",
    "    ticks = {}\n",
    "    latencies = []\n",
    "\n",
    "    def callback(y, request_id, sequence_id):\n",
    "        tock = time.time()\n",
    "        tick = ticks.pop(request_id)\n",
    "        latencies.append(tock - tick)\n",
    "        pbar.update(step_size / SAMPLE_RATE)\n",
    "\n",
    "    with ctx, pbar:\n",
    "        client = InferenceClient(\n",
    "            \"localhost:8001\",\n",
    "            model_name=\"aframe-stream\",\n",
    "            model_version=-1,\n",
    "            callback=callback\n",
    "        )\n",
    "        with client:\n",
    "            for i in range(min(num_steps, 10000)):\n",
    "                ticks[i] = time.time()\n",
    "                update = data[:, i * step_size: (i + 1) * step_size].astype(\"float32\")\n",
    "                client.infer(\n",
    "                    update,\n",
    "                    sequence_id=1001,\n",
    "                    request_id=i,\n",
    "                    sequence_start=i == 0,\n",
    "                    sequence_end=i == (num_steps - 1)\n",
    "                )\n",
    "                if i < 5:\n",
    "                    while i in ticks:\n",
    "                        time.sleep(0.01)\n",
    "                time.sleep(0.8 * batch_size / inference_sampling_rate)\n",
    "    return latencies\n",
    "\n",
    "\n",
    "def run_expt(batch_size, inference_sampling_rate, instances):\n",
    "    configure_deployment(batch_size, inference_sampling_rate, instances)\n",
    "    return do_inference(batch_size, inference_sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's where the actual heavy lifting happens. For each inference sampling rate, it will export a model, launch it in a Triton server, then do inference over it using a \"real-time\" request rate and measure the end-to-end latency for each request made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latencies = {}\n",
    "for i in range(5):\n",
    "    batch_size = 2**i\n",
    "    inference_sampling_rate = 2**(7 + i)\n",
    "    latencies[batch_size] = run_expt(batch_size, inference_sampling_rate, 6)\n",
    "# TODO: might make sense to save these out as you iterate on the plotting code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot all these measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(\n",
    "    height=300,\n",
    "    width=500,\n",
    "    x_axis_label=r\"$$\\text{Predictions per second [Hz]}$$\",\n",
    "    y_axis_label=r\"$$\\text{Expected trigger latency [s]}$$\",\n",
    "    x_axis_type=\"log\",\n",
    "    title=r\"$$\\text{Expected latency vs. predictive resolution}$$\"\n",
    ")\n",
    "p.toolbar_location = None\n",
    "p.title.text_font_style = \"normal\"\n",
    "\n",
    "for batch_size, measurements in latencies.items():\n",
    "    inference_sampling_rate = 2**7 * batch_size\n",
    "\n",
    "    # only take the last few thousand samples to allow\n",
    "    # server to stabilize after some burn-in\n",
    "    measurements = np.array(measurements[-8000:])\n",
    "\n",
    "    # add 1.5 to account for integration and whitening\n",
    "    # filter settle-in, then account for the average\n",
    "    # delay encountered by batch samples for having to\n",
    "    # wait for a full batch to generate (i.e. relative\n",
    "    # to when we _could_ have gotten an inference response\n",
    "    # if we were doing batch 1 inference)\n",
    "    latency = 1.5 + measurements + (batch_size - 1) / 2 / inference_sampling_rate\n",
    "    low, mid, high = np.percentile(latency, [5, 50, 95])\n",
    "\n",
    "    kwargs = {}\n",
    "    if batch_size == 1:\n",
    "        kwargs[\"legend_label\"] = \"90% interval\"\n",
    "\n",
    "    p.line(\n",
    "        [inference_sampling_rate, inference_sampling_rate],\n",
    "        [low, high],\n",
    "        line_color=\"#555555\",\n",
    "        line_width=1.2,\n",
    "        **kwargs\n",
    "    )\n",
    "    for val in [low, high]:\n",
    "        p.line(\n",
    "            [inference_sampling_rate * 0.98, inference_sampling_rate / 0.98],\n",
    "            [val, val],\n",
    "            line_color=\"#555555\",\n",
    "            line_width=1.2,\n",
    "        )\n",
    "\n",
    "    if kwargs:\n",
    "        kwargs[\"legend_label\"] = \"Median\"\n",
    "    p.circle(\n",
    "        inference_sampling_rate,\n",
    "        mid,\n",
    "        fill_alpha=0.5,\n",
    "        size=9,\n",
    "        **kwargs\n",
    "    )\n",
    "p.legend.location = \"bottom_right\"\n",
    "show(p)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
