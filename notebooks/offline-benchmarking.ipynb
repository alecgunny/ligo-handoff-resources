{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.palettes import Dark2_8 as palette\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"log/server-stats-128.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(\n",
    "    height=300,\n",
    "    width=600,\n",
    "    y_axis_label=\"Throughput\",\n",
    "    x_axis_label=\"Time [minutes]\",\n",
    "    tools=\"\"\n",
    ")\n",
    "p.toolbar_location = None\n",
    "\n",
    "# this will plot aggregate throughput of each model\n",
    "# in your inference deployment (in units of seconds\n",
    "# of data per second) as a function time. The aframe\n",
    "# model, since it's the last, will naturally be the\n",
    "# throughput that you observe. But you can look for\n",
    "# disparities to get a sense of which models are\n",
    "# acting as the bottleneck in your deployment.\n",
    "t0  = df.timestamp.min()\n",
    "for i, (model, subdf) in enumerate(df.groupby(\"model\")):\n",
    "    diff = subdf[\"timestamp\"].diff()[1:]\n",
    "    counts = subdf[\"count\"].iloc[1:]\n",
    "    inf_per_s = counts / diff\n",
    "    s_per_s = inf_per_s * 128 / 4\n",
    "\n",
    "    # you could also plot something similar by plotting\n",
    "    # something like\n",
    "    # queue = subdf[\"queue\"] / subdf[\"count\"]\n",
    "    # which would give you some insights into how long\n",
    "    # requests spent queuing for each model, which can\n",
    "    # give you some insight into bottlenecks as well.\n",
    "    t = (subdf[\"timestamp\"].values[1:] - t0) / 60\n",
    "\n",
    "    p.line(\n",
    "        t,\n",
    "        s_per_s,\n",
    "        line_width=1.5,\n",
    "        line_color=palette[i],\n",
    "        line_alpha=0.7,\n",
    "        legend_label=model\n",
    "    )\n",
    "show(p)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
