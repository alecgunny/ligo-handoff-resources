{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: run this notebook on a node with a GPU. See my ADASS demo for an environment that should be sufficient to run this and also has some plotting utils for making the output plot a bit prettier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "from bilby.core.prior import Cosine, PriorDict, Uniform\n",
    "from gwpy.timeseries import TimeSeriesDict\n",
    "from pycbc.detector import Detector\n",
    "from pycbc.filter.matchedfilter import sigmasq, make_frequency_series\n",
    "from pycbc.types import FrequencySeries, TimeSeries\n",
    "from pycbc.waveform import get_td_waveform\n",
    "from tqdm import trange\n",
    "\n",
    "# this assumes that you have data like the kind generated\n",
    "# by the ADASS demo. Should be straightforward to adapt\n",
    "# to the aframe data setup\n",
    "ifos = [\"H1\", \"L1\"]\n",
    "detectors = {i: Detector(i) for i in ifos}\n",
    "t0 = 1238175433\n",
    "kernel_length = 1.5\n",
    "fduration = 1\n",
    "sample_length = kernel_length + fduration\n",
    "\n",
    "with h5py.File(\"data/signals.hdf5\", \"r\") as f:\n",
    "    hp, hc = [f[\"train\"][\"polarizations\"][i][0].astype(np.float64) for i in [\"plus\", \"cross\"]]\n",
    "with h5py.File(\"data/background.hdf5\", \"r\") as f:\n",
    "    tsd = TimeSeriesDict.read(f[f\"train/{t0}-17136\"], path=ifos, start=t0, end=t0 + 10.5)\n",
    "\n",
    "sample_rate = tsd[ifos[0]].sample_rate\n",
    "signal_length = hp.shape[-1] / sample_rate\n",
    "\n",
    "\n",
    "class StopWatch:\n",
    "    def __init__(self):\n",
    "        self.intervals = {}\n",
    "\n",
    "    def __call__(self, f):\n",
    "        try:\n",
    "            name = f.__name__\n",
    "        except AttributeError:\n",
    "            name = f.__class__.__name__\n",
    "        self.intervals[name] = []\n",
    "\n",
    "        def wrapper(*args, **kwargs):\n",
    "            tick = time.time()\n",
    "            out = f(*args, **kwargs)\n",
    "            tock = time.time()\n",
    "            self.intervals[name].append(tock - tick)\n",
    "            return out\n",
    "        return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark the pycbc implementation. If you want to do bilby instead, should be straightforward to adapt to the bilby code (which would frankly probably look simpler). Start by defining the functions we'll benchmark.\n",
    "\n",
    "TODO: need to add injection and whitening step at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skyloc_prior = PriorDict(dict(\n",
    "    declination=Cosine(),\n",
    "    right_ascension=Uniform(0, 2 * np.pi),\n",
    "    polarization=Uniform(0, np.pi)\n",
    "))\n",
    "\n",
    "vanilla_timer = StopWatch()\n",
    "\n",
    "def _compute_psd(x):\n",
    "    xtilde = x.psd(fftlength=2, method=\"median\", window=\"hann\")\n",
    "    xtilde = xtilde.interpolate(1 / signal_length)\n",
    "    return FrequencySeries(xtilde.value, delta_f=1/signal_length)\n",
    "\n",
    "\n",
    "@vanilla_timer\n",
    "def compute_psds(x):\n",
    "    return  {k: _compute_psd(v.crop(t0, t0 + 8)) for k, v in x.items()}\n",
    "\n",
    "\n",
    "def _compute_ht(ifo, hp, hc, **skyloc):\n",
    "    fp, fc = detectors[ifo].antenna_pattern(t_gps=t0, **skyloc)\n",
    "    return TimeSeries(fp * hp + fc * hc, delta_t=1/2048)\n",
    "\n",
    "\n",
    "@vanilla_timer\n",
    "def compute_ht(hp, hc, **skyloc):\n",
    "    return {i: _compute_ht(i, hp, hc, **skyloc) for i in ifos}\n",
    "\n",
    "\n",
    "def _compute_snr(ht, psd, highpass=32):\n",
    "    htilde = make_frequency_series(ht)\n",
    "    return sigmasq(htilde, psd, low_frequency_cutoff=highpass)\n",
    "    \n",
    "\n",
    "@vanilla_timer\n",
    "def compute_snr(ht, psds, highpass=32):\n",
    "    return sum([_compute_snr(ht[i], psds[i], highpass) for i in ifos])**0.5\n",
    "\n",
    "\n",
    "def make_sample(hp, hc):\n",
    "    psds = compute_psds(tsd)\n",
    "    skyloc = skyloc_prior.sample()\n",
    "    ht = compute_ht(hp, hc, **skyloc)\n",
    "    target_snr = np.random.uniform(7, 100)\n",
    "    snr = compute_snr(ht, psds)\n",
    "    ht = {k: v * target_snr / snr for k, v in ht.items()}\n",
    "\n",
    "    # TODO: something like this but wrapped in a timer fn\n",
    "    # ht = {k: v.crop(t0 + 8, t0 + 10.5) + ht[k].value[SOME CROP] for k in tsd.items()}\n",
    "    # ht = {k: v.whiten(asd=psds[k]**0.5) for k, v in ht.items()}\n",
    "    return ht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now actually run this over 1000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trange(1000):\n",
    "    make_sample(hp, hc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the same functions but in `ml4gw` land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ml4gw import distributions, gw, transforms\n",
    "from ml4gw.utils.slicing import sample_kernels\n",
    "\n",
    "\n",
    "class Ml4gwAugmenter(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Model with additional methods for performing our\n",
    "    preprocessing augmentations in real-time on the GPU.\n",
    "    Also loads training background in chunks from disk,\n",
    "    then samples batches from chunks.\n",
    "\n",
    "    Note that the training and validation steps themselves\n",
    "    don't need to change at all: all we're doing is building\n",
    "    better ways of getting data to _feed_ to the training\n",
    "    step.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        ifos: list[str],\n",
    "        kernel_length: float,\n",
    "        fduration: float,\n",
    "        psd_length: float,\n",
    "        sample_rate: float,\n",
    "        fftlength: float,\n",
    "        highpass: float = 32\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # real-time transformations defined at torch Modules\n",
    "        self.spectral_density = transforms.SpectralDensity(\n",
    "            sample_rate, fftlength, average=\"median\", fast=True\n",
    "        )\n",
    "        self.whitener = transforms.Whiten(fduration, sample_rate, highpass=highpass)\n",
    "\n",
    "        # get some geometry information about\n",
    "        # the interferometers we're going to project to\n",
    "        detector_tensors, vertices = gw.get_ifo_geometry(*ifos)\n",
    "        self.register_buffer(\"detector_tensors\", detector_tensors)\n",
    "        self.register_buffer(\"detector_vertices\", vertices)\n",
    "\n",
    "        # define some sky parameter distributions\n",
    "        self.declination = distributions.Cosine()\n",
    "        self.polarization = distributions.Uniform(0, torch.pi)\n",
    "        self.phi = distributions.Uniform(-torch.pi, torch.pi)  # relative RAs of detector and source\n",
    "\n",
    "        # rather than sample distances, we'll sample target SNRs.\n",
    "        # This way we can ensure we train our network on\n",
    "        # signals that are actually detectable. We'll use a distribution\n",
    "        # that looks roughly like our sampled SNR distribution\n",
    "        self.snr = distributions.PowerLaw(4, 100, 3)\n",
    "\n",
    "        # up front let's define some properties in units of samples\n",
    "        self.kernel_size = int(kernel_length * sample_rate)\n",
    "        self.window_size = self.kernel_size + int(fduration * sample_rate)\n",
    "        self.psd_size = int(psd_length * sample_rate)\n",
    "\n",
    "    def project_waveforms(self, hc: torch.Tensor, hp: torch.Tensor) -> torch.Tensor:\n",
    "        # sample sky parameters\n",
    "        N = len(hc)\n",
    "        declination = self.declination(N).to(hc)\n",
    "        polarization = self.polarization(N).to(hc)\n",
    "        phi = self.phi(N).to(hc)\n",
    "\n",
    "        # project to interferometer response\n",
    "        return gw.compute_observed_strain(\n",
    "            declination,\n",
    "            polarization,\n",
    "            phi,\n",
    "            detector_tensors=self.detector_tensors,\n",
    "            detector_vertices=self.detector_vertices,\n",
    "            sample_rate=self.hparams.sample_rate,\n",
    "            cross=hc,\n",
    "            plus=hp\n",
    "        )\n",
    "\n",
    "    def rescale_snrs(self, responses: torch.Tensor, psd: torch.Tensor) -> torch.Tensor:\n",
    "        # make sure everything has the same number of frequency bins\n",
    "        num_freqs = int(responses.size(-1) // 2) + 1\n",
    "        if psd.size(-1) != num_freqs:\n",
    "            psd = torch.nn.functional.interpolate(psd, size=(num_freqs,), mode=\"linear\")\n",
    "        snrs = gw.compute_network_snr(\n",
    "            responses.double(), psd, self.hparams.sample_rate, self.hparams.highpass\n",
    "        )\n",
    "\n",
    "        N = len(responses)\n",
    "        target_snrs = self.snr(N).to(snrs.device)\n",
    "        weights = target_snrs / snrs\n",
    "        return responses * weights.view(-1, 1, 1)\n",
    "\n",
    "    def sample_kernels(self, responses: torch.Tensor) -> torch.Tensor:\n",
    "        # slice off random views of each waveformto inject in arbitrary positions\n",
    "        responses = responses[:, :, -self.window_size:]\n",
    "\n",
    "        # pad so that at least half the kernel always contains signals\n",
    "        pad = [0, int(self.window_size // 2)]\n",
    "        responses = torch.nn.functional.pad(responses, pad)\n",
    "        return sample_kernels(responses, self.window_size, coincident=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the module and move it to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ml4gwAugmenter(\n",
    "    ifos,\n",
    "    kernel_length,\n",
    "    fduration,\n",
    "    psd_length=8,\n",
    "    sample_rate=sample_rate,\n",
    "    highpas=32\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that will run through the relevant benchmarking as a function of batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: this also reflects the ADASS demo data structure. Again,\n",
    "# should be trivial to transition to the aframe data structure\n",
    "def get_polarization(f, polar, batch_size):\n",
    "    return torch.Tensor(f[\"train/polarizations\"][polar][:batch_size]).to(\"cuda\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def ml4gw_benchmark(batch_size, N=1000):\n",
    "    X = np.stack([tsd[i].value for i in ifos])\n",
    "    X = np.stack([X for _ in range(batch_size)])\n",
    "    X = torch.Tensor(X).to(\"cuda\")\n",
    "\n",
    "    with h5py.File(\"data/signals.hdf5\", \"r\") as f:\n",
    "        hp = get_polarization(f, \"plus\", batch_size)\n",
    "        hc = get_polarization(f, \"cross\", batch_size)\n",
    "\n",
    "    ml4gw_timer = StopWatch()\n",
    "    spec = ml4gw_timer(model.spectral_density)\n",
    "    proj = ml4gw_timer(model.project_waveforms)\n",
    "    resc = ml4gw_timer(model.rescale_snrs)\n",
    "    for _ in trange(N):\n",
    "        psds = spec(X)\n",
    "        responses = proj(hc, hp)\n",
    "        responses = resc(responses, psds)\n",
    "        # TODO: add slice, add, run through whitener\n",
    "    return ml4gw_timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the benchmarking over the various batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "mean, var = defaultdict(dict), defaultdict(dict)\n",
    "for batch_size in [32, 128, 512, 2048]:\n",
    "    print(batch_size)\n",
    "    timer = ml4gw_benchmark(batch_size)\n",
    "    for k, v in timer.intervals.items():\n",
    "        mean[k][batch_size] = np.mean(v)\n",
    "        var[k][batch_size] = np.var(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot everything in a logarithmic bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import FactorRange\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.palette import Bright8 as palette\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "factors = [\"CPU, no batching\"] + [f\"GPU, batch size {i}\" for i in [32, 128, 512, 2048]]\n",
    "\n",
    "# TODO: feel free to use some of my ADASS plotting utils\n",
    "# to make this figure prettier\n",
    "p = figure(\n",
    "    x_range=FactorRange(*factors),\n",
    "    background_fill_alpha=1.0,\n",
    "    x_axis_label=r\"\\text{Implementation}\",\n",
    "    y_axis_label=r\"\\text{Average Execution Time [ms]}\",\n",
    "    y_axis_type=\"log\"\n",
    ")\n",
    "\n",
    "maps = [\n",
    "    (\"SpectralDensity\", \"compute_psds\", \"Compute PSD\"),\n",
    "    (\"project_waveforms\", \"compute_ht\", \"Project Waveforms\"),\n",
    "    (\"rescale_snrs\", \"compute_snr\", \"Rescale SNR\")\n",
    "]\n",
    "data = dict(impl=factors)\n",
    "for i in maps:\n",
    "    data[i[-1]] = []\n",
    "\n",
    "x = []\n",
    "bottom = []\n",
    "top = []\n",
    "labels = []\n",
    "colors = []\n",
    "for factor in factors:\n",
    "    start = 0.1\n",
    "    if factor.startswith(\"CPU\"):\n",
    "        for map, color in zip(maps, palette[-2::-1]):\n",
    "            x.append(factor)\n",
    "            bottom.append(start)\n",
    "            t = np.mean(vanilla_timer.intervals[map[1]]) * 1000\n",
    "            top.append(start + t)\n",
    "            labels.append(map[2])\n",
    "            start += t\n",
    "            colors.append(color)\n",
    "    else:\n",
    "        for map, color in zip(maps, palette[-2::-1]):\n",
    "            bs = int(factor.split()[-1])\n",
    "            x.append(factor)\n",
    "            bottom.append(start)\n",
    "            t = mean[map[0]][bs] * 1000 / bs\n",
    "            top.append(start + t)\n",
    "            labels.append(map[2])\n",
    "            start += t\n",
    "            colors.append(color)\n",
    "source = dict(\n",
    "    x=x,\n",
    "    top=top,\n",
    "    bottom=bottom,\n",
    "    label=labels,\n",
    "    color=colors\n",
    ")\n",
    "p.vbar(\n",
    "    x=\"x\",\n",
    "    top=\"top\",\n",
    "    bottom=\"bottom\",\n",
    "    width=0.9,\n",
    "    legend_field=\"label\",\n",
    "    fill_color=\"color\",\n",
    "    fill_alpha=0.8,\n",
    "    line_color=\"#333333\",\n",
    "    line_width=0.5,\n",
    "    source=source\n",
    ")\n",
    "show(p)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
